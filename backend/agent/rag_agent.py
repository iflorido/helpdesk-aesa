"""
Agente RAG que combina bÃºsqueda en documentos con LLM.
"""
from typing import List, Dict, Optional
import logging

from agent.llm_client import get_llm_client
from rag import get_vector_store

logger = logging.getLogger(__name__)


SYSTEM_PROMPT = """Eres un asistente experto en normativa AESA (Agencia Estatal de Seguridad AÃ©rea) para drones, especializado en las categorÃ­as A1, A2 y A3.

Tu objetivo es ayudar a los usuarios con consultas sobre:
- Requisitos y limitaciones de cada categorÃ­a
- Distancias de seguridad
- Zonas permitidas y restringidas
- Procedimientos operacionales
- FormaciÃ³n y certificaciÃ³n necesarias

INSTRUCCIONES IMPORTANTES:
1. Basa tus respuestas ÃšNICAMENTE en la informaciÃ³n proporcionada en el contexto de los documentos AESA
2. Si la informaciÃ³n no estÃ¡ en el contexto, di claramente "No encuentro esa informaciÃ³n en la documentaciÃ³n que tengo disponible"
3. Cita siempre la fuente cuando sea posible (ej: "SegÃºn el reglamento AESA...")
4. SÃ© claro, preciso y conciso
5. Si detectas que la consulta requiere intervenciÃ³n humana (casos muy especÃ­ficos, interpretaciones legales complejas), sugiÃ©relo
6. Usa un tono profesional pero amigable

Recuerda: La seguridad aÃ©rea es prioritaria, asÃ­ que es mejor ser conservador en las respuestas que arriesgarse a dar informaciÃ³n incorrecta."""


class RAGAgent:
    """Agente que combina RAG con LLM para responder consultas."""
    
    def __init__(self):
        """Inicializa el agente RAG."""
        self.llm = get_llm_client()
        self.vector_store = get_vector_store()
        logger.info("âœ… Agente RAG inicializado")
    
    def search_relevant_context(
        self,
        query: str,
        n_results: int = 5,
        document_type: Optional[str] = None
    ) -> tuple[str, List[Dict]]:
        """
        Busca contexto relevante en los documentos.
        
        Args:
            query: Consulta del usuario
            n_results: NÃºmero de resultados a buscar
            document_type: Filtrar por tipo de documento (opcional)
        
        Returns:
            Tupla de (contexto_combinado, fuentes)
        """
        logger.info(f"ğŸ” Buscando contexto para: '{query[:100]}...'")
        
        # Buscar en el vector store
        where_filter = {"document_type": document_type} if document_type else None
        
        results = self.vector_store.search(
            query=query,
            n_results=n_results,
            where=where_filter
        )
        
        # Combinar los documentos encontrados
        documents = results["documents"][0]
        metadatas = results["metadatas"][0]
        distances = results["distances"][0]
        
        if not documents:
            logger.warning("âš ï¸ No se encontraron documentos relevantes")
            return "", []
        
        # Crear contexto combinado
        context_parts = []
        sources = []
        
        for i, (doc, meta, dist) in enumerate(zip(documents, metadatas, distances)):
            relevance = 1 - dist
            
            context_parts.append(
                f"--- Fragmento {i+1} (Relevancia: {relevance:.0%}) ---\n"
                f"Fuente: {meta.get('source', 'Desconocida')}\n"
                f"{doc}\n"
            )
            
            sources.append({
                "source": meta.get('source', 'Desconocida'),
                "document_type": meta.get('document_type', 'unknown'),
                "relevance": relevance,
                "chunk_index": meta.get('chunk_index', 0)
            })
        
        context = "\n".join(context_parts)
        
        logger.info(f"âœ… Encontrados {len(documents)} fragmentos relevantes")
        
        return context, sources
    
    def generate_response(
        self,
        user_query: str,
        conversation_history: Optional[List[Dict]] = None,
        document_type: Optional[str] = None
    ) -> Dict:
        """
        Genera una respuesta usando RAG + LLM.
        
        Args:
            user_query: Pregunta del usuario
            conversation_history: Historial previo de la conversaciÃ³n
            document_type: Filtrar bÃºsqueda por tipo de documento
        
        Returns:
            Diccionario con content, metadata y sources
        """
        logger.info(f"ğŸ’¬ Generando respuesta para: '{user_query[:100]}...'")
        
        # 1. Buscar contexto relevante
        context, sources = self.search_relevant_context(
            query=user_query,
            n_results=5,
            document_type=document_type
        )
        
        # 2. Construir mensajes para el LLM
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT}
        ]
        
        # AÃ±adir historial de conversaciÃ³n si existe
        if conversation_history:
            for msg in conversation_history[-10:]:  # Ãšltimos 10 mensajes
                messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
        
        # AÃ±adir contexto y consulta actual
        if context:
            user_message = f"""CONTEXTO DE DOCUMENTOS AESA:
{context}

---

CONSULTA DEL USUARIO:
{user_query}

Responde basÃ¡ndote en el contexto proporcionado."""
        else:
            user_message = f"""CONSULTA DEL USUARIO:
{user_query}

NOTA: No se encontrÃ³ informaciÃ³n especÃ­fica en los documentos. Responde indicando que no tienes esa informaciÃ³n disponible y sugiere consultar con AESA directamente."""
        
        messages.append({"role": "user", "content": user_message})
        
        # 3. Generar respuesta con el LLM
        llm_response = self.llm.chat_completion(
            messages=messages,
            temperature=0.7,
            max_tokens=1000
        )
        
        # 4. Construir respuesta completa
        response = {
            "content": llm_response["content"],
            "sources": sources,
            "metadata": {
                **llm_response["metadata"],
                "sources_count": len(sources),
                "has_context": bool(context)
            }
        }
        
        logger.info(f"âœ… Respuesta generada. Tokens: {response['metadata']['tokens_total']}")
        
        return response
    
    def should_escalate(self, response: Dict) -> tuple[bool, str]:
        """
        Determina si la consulta debe escalarse a un humano.
        
        Args:
            response: Respuesta generada
        
        Returns:
            Tupla de (should_escalate, reason)
        """
        content = response["content"].lower()
        sources_count = response["metadata"]["sources_count"]
        
        # Reglas de escalado
        if sources_count == 0:
            return True, "No se encontrÃ³ informaciÃ³n relevante en la documentaciÃ³n"
        
        if "no encuentro" in content or "no tengo informaciÃ³n" in content:
            return True, "El agente no pudo encontrar informaciÃ³n especÃ­fica"
        
        if "consulta con aesa" in content or "contacta con" in content:
            return True, "La consulta requiere verificaciÃ³n oficial"
        
        # Palabras clave que indican casos complejos
        complex_keywords = ["legal", "demanda", "accidente", "sanciÃ³n", "multa"]
        if any(keyword in content for keyword in complex_keywords):
            return True, "Consulta de naturaleza legal o compleja"
        
        return False, ""


# Instancia global del agente
_rag_agent = None

def get_rag_agent() -> RAGAgent:
    """
    Dependency para obtener el agente RAG.
    Usa singleton pattern.
    """
    global _rag_agent
    
    if _rag_agent is None:
        _rag_agent = RAGAgent()
    
    return _rag_agent
